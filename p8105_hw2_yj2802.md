## Problem 1

The dataset “Subway” contains information related to each entrance and
exit for each subway station in NYC. Before starting to analyze, it’s
necessary to clean the data by the following steps:

1.  Retain: (1)line; (2)station name; (3)station latitude; (4)station
    longitude; (5)routes served; (6)entry; (7)vending; (8)entrance type;
    and (9)ADA compliance.

2.  Convert the entry variable from character to a logical variable.

<!-- -->

    library(tidyverse)

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

    subway = read_csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/Hws/Hw2/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    #################################
    ##1.Retain columns
    P1 = subway |>
    select(`Station Name`, Line, Route1, Route2, Route3, Route4, Route5, Route6, Route7, Route8, Route8, Route10, Route11, ADA, Vending, Entry)
    ##2.Convert the entry variable
    P1 = P1 |> mutate(Entry = ifelse(Entry == "YES", TRUE, FALSE))
    #################################
    ##Distinct stations:
    distinct_stations = P1 |> distinct(`Station Name`, Line)
    ##ADA compliant stations:
    ada_compliant_stations = P1 |>
    filter(ADA == TRUE) |>
    distinct(`Station Name`, Line)
    ##proportion of station entrances/exits without vending allow entrance:
    no_vending = subway |> filter(Vending == "NO")
    proportion_no_vending_entry = no_vending |> summarise(proportion = mean(Entry, na.rm = TRUE))

    ## Warning: There was 1 warning in `summarise()`.
    ## ℹ In argument: `proportion = mean(Entry, na.rm = TRUE)`.
    ## Caused by warning in `mean.default()`:
    ## ! argument is not numeric or logical: returning NA

    formatted_percentage = sprintf("%.2f%%", proportion_no_vending_entry$proportion * 100)

After cleaning the data, the subway dataset now has 1868 rows and 15
columns. Including the following variables: Station Name, Line, Route1,
Route2, Route3, Route4, Route5, Route6, Route7, Route8, Route10,
Route11, ADA, Vending, Entry. There are 465 distinct stations in total,
and 84 stations are ADA compliant. Besides, there are NA% station
entrances / exits have no vending allow entrance.

## Problem 2

The dataset is about 3 water-wheel vessels named “Mr. Trash Wheel”,
“Professor Trash Wheel”, and “Gwynnda Trash Wheel” that removes trash
from the Inner Harbor in Baltimore. Firstly, clean the 3 sheets by the
following steps:

    # Load necessary libraries
    library(readxl)
    library(dplyr)
    # Define file path
    file_path = "C:/Users/22865/OneDrive/Desktop/Data Science 1/Hws/Hw2/202409 Trash Wheel Collection Data.xlsx"
    #################################
    ##Import the "Mr. Trash Wheel sheet" and clean the data:
    mr_trash_wheel = read_excel(file_path, 
                                 sheet = "Mr. Trash Wheel", 
                                 col_types = c("numeric",   # Dumpstevier
                                               "text",      # Month
                                               "text",   # Year
                                               "date",      # Date
                                               "numeric",   # Weight (tons)
                                               "numeric",   # Volume (cubic yards)
                                               "numeric",   # Plastic Bottles
                                               "numeric",   # Polystyrene
                                               "numeric",   # Cigarette Butts
                                               "numeric",   # Glass Bottles
                                               "numeric",   # Plastic Bags
                                               "numeric",   # Wrappers
                                               "numeric",   # Sports Balls
                                               "numeric",    # Homes Powered
                                               "skip",      # Ignoring 15th column
                                               "skip"       # Ignoring 16th column
                                               ), 
                                skip =1) |>
      janitor::clean_names() |> 
      drop_na(dumpster) |>
      mutate(sports_balls = as.integer(round(sports_balls)))

    ########################
    prof_trash_wheel = read_excel(file_path, 
                                 sheet = "Professor Trash Wheel", 
                                 skip =1, 
                                 col_types = c("numeric",   # Dumpster
                                               "text",      # Month
                                               "text",   # Year
                                               "date",      # Date
                                               "numeric",   # Weight (tons)
                                               "numeric",   # Volume (cubic yards)
                                               "numeric",   # Plastic Bottles
                                               "numeric",   # Polystyrene
                                               "numeric",   # Cigarette Butts
                                               "numeric",   # Glass Bottles
                                               "numeric",   # Plastic Bags
                                               "numeric",   # Wrappers
                                               "numeric"    # Homes Powered
                                               )) |>
      janitor::clean_names() |> 
      drop_na(dumpster)
    ########################
    Gwynnda_trash_wheel = read_excel(file_path, 
                                 sheet = "Gwynnda Trash Wheel", 
                                 skip =1, 
                                 col_types = c("numeric",   # Dumpster
                                               "text",      # Month
                                               "text",   # Year
                                               "date",      # Date
                                               "numeric",   # Weight (tons)
                                               "numeric",   # Volume (cubic yards)
                                               "numeric",   # Plastic Bottles
                                               "numeric",   # Polystyrene
                                               "numeric",   # Cigarette Butts
                                               "numeric",   # Plastic Bags
                                               "numeric",   # Wrappers
                                               "numeric"    # Homes Powered
                                               )) |>
      janitor::clean_names() |> 
      drop_na(dumpster)

1.  Combine 3 datasets

<!-- -->

    #Add additional variables:
    mr_trash_wheel = mr_trash_wheel |> mutate(trash_wheel = "mr")
    prof_trash_wheel = prof_trash_wheel |>mutate(trash_wheel = "prof")
    Gwynnda_trash_wheel = Gwynnda_trash_wheel |> mutate(Gwynnda_trash_wheel, trash_wheel = "Gwynnda")
    #Combine datasets:
    combined_data = bind_rows(mr_trash_wheel, prof_trash_wheel, Gwynnda_trash_wheel)

The combined dataset includes 1033 observations from Mr. Trash Wheel,
Professor Trash Wheel and Gwynnda Trash Wheel. There are 15 variables in
total. The key variables in the dataset include Dumpster, Weight (tons),
Sports Balls et al.

Additional analysis:

    ##Calculate the total weight of trash collected by Professor Trash Wheel
    total_weight <- sum(prof_trash_wheel$weight_tons, na.rm = TRUE)
    ##Calculate the total cigarette butts collected by Gwynnnda in June 2022
    total_cig_butts = Gwynnda_trash_wheel %>%
    filter(month == "June" & year == 2022) %>%
    summarise(total_cig_butts = sum(cigarette_butts, na.rm = TRUE))

Through further analysis by using the code above, we can obtain the
following result:

1.  The total weight of trash collected by Professor Trash Wheel is
    246.74 tons.

2.  The total number of cigarette butts collected by Gwynnda in June of
    2022 is 1.812^{4}.

## Problem 3

This problem uses data on elements of the Great British Bake Off.
Information about individual bakers, their bakes, and their performance
is included in bakers.csv, bakes.csv, and results.csv.

1.  The first step is to create a single, well-organized final dataset
    with all the information contained in these data files by the
    following steps:

(1)) Specify the column types by read\_excel(col\_types()).

(2)) Clean column names by janitor::clean\_names().

(3)) Rename columns in results (x, x\_1, et al.to specific names).

(4)) Extract first names: Create a baker column in bakers by extracting
the first name from baker\_name.

(5)) Remove double quotes: Remove double quotes from the baker column in
bakes.

(6)) Use anti\_join() in multiple times to make sure there’s no missing
row.

(7)) Merge 3 datasets.

(8)) Clean the merged dataset: Select key variables, filter out rows
where result is NA.

(9)) Export the final dataset.

Relevant code below:

    library(dplyr)
    library(tidyr)
    bakers = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/bakers.csv",colClasses = c("character",  # Baker Name
                                      "character",  # Series
                                      "integer",    # Baker Age
                                      "character",  # Baker Occupation
                                      "character"   # Hometown
                                      ))
    bakes = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/bakes.csv", 
                      colClasses = c("character",  # Series
                                     "character",  # Episode
                                     "character",  # Baker
                                     "character",  # Signature Bake
                                     "character"   # Show Stopper
                                     ))
    results = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/results.csv", skip = 1,
                        colClasses = c("character",  # Series
                                       "character",  # Episode
                                       "character",  # Baker
                                       "character",  # Technical
                                       "character"   # Result
                                       ))
    viewers = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/viewers.csv", 
                        colClasses = c("character",  # Episode
                                       rep("numeric", 10)
                                       ))
    bakers = janitor::clean_names(bakers)
    bakes = janitor::clean_names(bakes)
    results = janitor::clean_names(results)
    viewers = janitor::clean_names(viewers)
    colnames(results)

    ## [1] "x"   "x_1" "x_2" "x_3" "x_4"

    # Rename columns from results:
    results = results |>
    rename(series = x, episode = x_1, baker = x_2, technical=x_3, result = x_4)
    # Create variable "baker" as the first name in Bakers dataset:
    bakers = bakers |> mutate(baker = sub(" .*", "", `baker_name`)) 
    # Delete "" of "Jo" in Bakes dataset:
    bakes = bakes |> mutate(baker = gsub('"', '', baker))
    # Check the datasets:
    missing_bakers <- anti_join(bakes, bakers, by = c("series", "baker"))
    print(missing_bakers) #No missing row here#

    ## [1] series         episode        baker          signature_bake show_stopper  
    ## <0 rows> (or 0-length row.names)

    # Merge the datasets:
    merged = bakes |>
    left_join(bakers, by = c("baker", "series"))
    merged = merged |>
    left_join(results, by = c("series", "episode", "baker"))
    # Clean and tidy the dataset:
    final = merged |>
    select(series, episode, baker_name, baker_age, hometown, baker_occupation,  signature_bake, show_stopper, result) |>
    filter(!is.na(result))
    # Export the final cleaned dataset
    write.csv(final, "final_bakeoff_data.csv", row.names = FALSE)

The final dataset, “final\_bakeoff\_data”, contains key information from
the three merged tables: bakers, bakes, and results which has 9
variables and 540 observations. The baker\_name, baker\_age, hometown,
and baker\_occupation columns provide background on each contestant,
while signature\_bake, show\_stopper, and result summarize their
performance. All columns have consistent data types after processing.
Rows with missing result values were filtered out, ensuring that only
complete records are included. The dataset is clean and ready for
analysis, with relevant details from each stage of the bake-off
competition.

1.  Create a table showing the star baker or winner of each episode in
    Seasons 5 through 10:

<!-- -->

    # Filter the dataset for Seasons 5 through 10
    seasons_5_to_10 = final |> filter(as.numeric(series) >= 5 & as.numeric(series) <= 10)
    # Extract episodes where the result is "Star Baker" or "Winner"
    star_baker_winner <- seasons_5_to_10 %>%
    filter(result %in% c("STAR BAKER", "WINNER")) %>%
    select(series, episode, baker_name, result) %>%
    arrange(series, episode)
    print(star_baker_winner)

    ##    series episode           baker_name     result
    ## 1       5       1    Nancy Birtwhistle STAR BAKER
    ## 2       5      10    Nancy Birtwhistle     WINNER
    ## 3       5       2         Richard Burr STAR BAKER
    ## 4       5       3         Luis Troyano STAR BAKER
    ## 5       5       4         Richard Burr STAR BAKER
    ## 6       5       5           Kate Henry STAR BAKER
    ## 7       5       6         Chetna Makan STAR BAKER
    ## 8       5       7         Richard Burr STAR BAKER
    ## 9       5       8         Richard Burr STAR BAKER
    ## 10      5       9         Richard Burr STAR BAKER
    ## 11      6       1       Marie Campbell STAR BAKER
    ## 12      6      10       Nadiya Hussain     WINNER
    ## 13      6       2          Ian Cumming STAR BAKER
    ## 14      6       3          Ian Cumming STAR BAKER
    ## 15      6       4          Ian Cumming STAR BAKER
    ## 16      6       5       Nadiya Hussain STAR BAKER
    ## 17      6       6            Mat Riley STAR BAKER
    ## 18      6       7            Tamal Ray STAR BAKER
    ## 19      6       8       Nadiya Hussain STAR BAKER
    ## 20      6       9       Nadiya Hussain STAR BAKER
    ## 21      7       1          Jane Beedle STAR BAKER
    ## 22      7      10        Candice Brown     WINNER
    ## 23      7       2        Candice Brown STAR BAKER
    ## 24      7       3        Tom Gilliford STAR BAKER
    ## 25      7       4     Benjamina Ebuehi STAR BAKER
    ## 26      7       5        Candice Brown STAR BAKER
    ## 27      7       6        Tom Gilliford STAR BAKER
    ## 28      7       7         Andrew Smyth STAR BAKER
    ## 29      7       8        Candice Brown STAR BAKER
    ## 30      7       9         Andrew Smyth STAR BAKER
    ## 31      8       1 Steven Carter-Bailey STAR BAKER
    ## 32      8      10         Sophie Faldo     WINNER
    ## 33      8       2 Steven Carter-Bailey STAR BAKER
    ## 34      8       3   Julia Chernogorova STAR BAKER
    ## 35      8       4            Kate Lyon STAR BAKER
    ## 36      8       5         Sophie Faldo STAR BAKER
    ## 37      8       6         Liam Charles STAR BAKER
    ## 38      8       7 Steven Carter-Bailey STAR BAKER
    ## 39      8       8          Stacey Hart STAR BAKER
    ## 40      8       9         Sophie Faldo STAR BAKER

Comments:

In Seasons 6, 7, and 8, the winners (Nadiya Hussain, Candice Brown, and
Sophie Faldo) each had multiple “Star Baker” titles, showing strong
performances throughout their seasons. Their consistent wins,
particularly in the later episodes, made their overall victories
predictable.

The biggest surprise was in Season 5, where Richard Burr won “Star
Baker” five times but lost the competition to Nancy Birtwhistle, who
only had one “Star Baker” win. Despite Richard’s dominance, the final
outcome was unexpected.

1.  Import, clean, tidy, and organize the viewership data in
    viewers.csv:

<!-- -->

    library(dplyr)
    library(tidyr)
    viewers = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/viewers.csv", 
                       colClasses = c("character", rep("numeric", 10)))
    viewers = janitor::clean_names(viewers)
    # Pivot the data to longer format for easier analysis:
    tidy_viewers = viewers |>
      pivot_longer(cols = starts_with("series"),
                   names_to = "series",
                   names_prefix = "series_",
                   values_to = "viewership")
    # Convert episode to numeric for better sorting:
    tidy_viewers = tidy_viewers |> mutate(episode = as.numeric(episode))
    # Organize the data by series and episode:
    organized_viewers = tidy_viewers |> arrange(as.numeric(series), episode)
    # Display the first 10 rows:
    head(organized_viewers, 10)

    ## # A tibble: 10 × 3
    ##    episode series viewership
    ##      <dbl> <chr>       <dbl>
    ##  1       1 1            2.24
    ##  2       2 1            3   
    ##  3       3 1            3   
    ##  4       4 1            2.6 
    ##  5       5 1            3.03
    ##  6       6 1            2.75
    ##  7       7 1           NA   
    ##  8       8 1           NA   
    ##  9       9 1           NA   
    ## 10      10 1           NA

    # Calculate average viewership for Season 1
    average_viewership_season_1 = organized_viewers |>
    filter(series == "1") |>
    summarise(avg_viewership = mean(viewership, na.rm = TRUE))
    # Calculate average viewership for Season 5
    average_viewership_season_5 = organized_viewers |>
    filter(series == "5") |>
    summarise(avg_viewership = mean(viewership, na.rm = TRUE))

By analyzing the organized viewers dataset:

The average viewership in Season 1 is 2.77, and is 10.0393 in Season 5.
