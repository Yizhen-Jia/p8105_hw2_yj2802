---
title: "p8105_hw2_yj2802"
author: "Yizhen Jia"
date: "`r Sys.Date()`"
output: 
  md_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1
The dataset "Subway" contains information related to each entrance and exit for each subway station in NYC. Before starting to analyze, it's necessary to clean the data by the following steps:
  
1. Retain: (1)line; (2)station name; (3)station latitude; (4)station longitude; (5)routes served; (6)entry; (7)vending; (8)entrance type; and (9)ADA compliance. 
  
2. Convert the entry variable from character to a logical variable.
```{r}
library(tidyverse)
subway = read_csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/Hws/Hw2/NYC_Transit_Subway_Entrance_And_Exit_Data.csv")
#################################
##1.Retain columns
P1 = subway |>
select(`Station Name`, Line, Route1, Route2, Route3, Route4, Route5, Route6, Route7, Route8, Route8, Route10, Route11, ADA, Vending, Entry)
##2.Convert the entry variable
P1 = P1 |> mutate(Entry = ifelse(Entry == "YES", TRUE, FALSE))
#################################
##Distinct stations:
distinct_stations = P1 |> distinct(`Station Name`, Line)
##ADA compliant stations:
ada_compliant_stations = P1 |>
filter(ADA == TRUE) |>
distinct(`Station Name`, Line)
##proportion of station entrances/exits without vending allow entrance:
no_vending = subway |> filter(Vending == "NO")
proportion_no_vending_entry = no_vending |> summarise(proportion = mean(Entry, na.rm = TRUE))
formatted_percentage = sprintf("%.2f%%", proportion_no_vending_entry$proportion * 100)
```
After cleaning the data, the subway dataset now has `r nrow(P1)` rows and `r ncol(P1)` columns. Including the following variables: `r names(P1)`. There are `r nrow(distinct_stations)` distinct stations in total, and `r nrow(ada_compliant_stations)` stations are ADA compliant. Besides, there are `r formatted_percentage` station entrances / exits have no vending allow entrance.

## Problem 2
The dataset is about 3 water-wheel vessels named "Mr. Trash Wheel", "Professor Trash Wheel", and "Gwynnda Trash Wheel" that removes trash from the Inner Harbor in Baltimore. Firstly, clean the 3 sheets by the following steps:
```{r}
# Load necessary libraries
library(readxl)
library(dplyr)
# Define file path
file_path = "C:/Users/22865/OneDrive/Desktop/Data Science 1/Hws/Hw2/202409 Trash Wheel Collection Data.xlsx"
#################################
##Import the "Mr. Trash Wheel sheet" and clean the data:
mr_trash_wheel = read_excel(file_path, 
                             sheet = "Mr. Trash Wheel", 
                             col_types = c("numeric",   # Dumpstevier
                                           "text",      # Month
                                           "text",   # Year
                                           "date",      # Date
                                           "numeric",   # Weight (tons)
                                           "numeric",   # Volume (cubic yards)
                                           "numeric",   # Plastic Bottles
                                           "numeric",   # Polystyrene
                                           "numeric",   # Cigarette Butts
                                           "numeric",   # Glass Bottles
                                           "numeric",   # Plastic Bags
                                           "numeric",   # Wrappers
                                           "numeric",   # Sports Balls
                                           "numeric",    # Homes Powered
                                           "skip",      # Ignoring 15th column
                                           "skip"       # Ignoring 16th column
                                           ), 
                            skip =1) |>
  janitor::clean_names() |> 
  drop_na(dumpster) |>
  mutate(sports_balls = as.integer(round(sports_balls)))

########################
prof_trash_wheel = read_excel(file_path, 
                             sheet = "Professor Trash Wheel", 
                             skip =1, 
                             col_types = c("numeric",   # Dumpster
                                           "text",      # Month
                                           "text",   # Year
                                           "date",      # Date
                                           "numeric",   # Weight (tons)
                                           "numeric",   # Volume (cubic yards)
                                           "numeric",   # Plastic Bottles
                                           "numeric",   # Polystyrene
                                           "numeric",   # Cigarette Butts
                                           "numeric",   # Glass Bottles
                                           "numeric",   # Plastic Bags
                                           "numeric",   # Wrappers
                                           "numeric"    # Homes Powered
                                           )) |>
  janitor::clean_names() |> 
  drop_na(dumpster)
########################
Gwynnda_trash_wheel = read_excel(file_path, 
                             sheet = "Gwynnda Trash Wheel", 
                             skip =1, 
                             col_types = c("numeric",   # Dumpster
                                           "text",      # Month
                                           "text",   # Year
                                           "date",      # Date
                                           "numeric",   # Weight (tons)
                                           "numeric",   # Volume (cubic yards)
                                           "numeric",   # Plastic Bottles
                                           "numeric",   # Polystyrene
                                           "numeric",   # Cigarette Butts
                                           "numeric",   # Plastic Bags
                                           "numeric",   # Wrappers
                                           "numeric"    # Homes Powered
                                           )) |>
  janitor::clean_names() |> 
  drop_na(dumpster)
```

3. Combine 3 datasets
```{r}
#Add additional variables:
mr_trash_wheel = mr_trash_wheel |> mutate(trash_wheel = "mr")
prof_trash_wheel = prof_trash_wheel |>mutate(trash_wheel = "prof")
Gwynnda_trash_wheel = Gwynnda_trash_wheel |> mutate(Gwynnda_trash_wheel, trash_wheel = "Gwynnda")
#Combine datasets:
combined_data = bind_rows(mr_trash_wheel, prof_trash_wheel, Gwynnda_trash_wheel)
```

The combined dataset includes `r nrow(combined_data)` observations from Mr. Trash Wheel, Professor Trash Wheel and Gwynnda Trash Wheel. There are `r ncol(combined_data)` variables in total. The key variables in the dataset include Dumpster, Weight (tons), Sports Balls et al.


Additional analysis:
```{r}
##Calculate the total weight of trash collected by Professor Trash Wheel
total_weight <- sum(prof_trash_wheel$weight_tons, na.rm = TRUE)
##Calculate the total cigarette butts collected by Gwynnnda in June 2022
total_cig_butts = Gwynnda_trash_wheel %>%
filter(month == "June" & year == 2022) %>%
summarise(total_cig_butts = sum(cigarette_butts, na.rm = TRUE))
```
Through further analysis by using the code above, we can obtain the following result:

1. The total weight of trash collected by Professor Trash Wheel is `r total_weight` tons. 

2. The total number of cigarette butts collected by Gwynnda in June of 2022 is `r total_cig_butts`.


## Problem 3
This problem uses data on elements of the Great British Bake Off. Information about individual bakers, their bakes, and their performance is included in bakers.csv, bakes.csv, and results.csv. 

1. The first step is to create a single, well-organized final dataset with all the information contained in these data files by the following steps:

 (1)) Specify the column types by read_excel(col_types()).
 
 (2)) Clean column names by janitor::clean_names().
 
 (3)) Rename columns in results (x, x_1, et al.to specific names).
 
 (4)) Extract first names: Create a baker column in bakers by extracting the first name from baker_name.
 
 (5)) Remove double quotes: Remove double quotes from the baker column in bakes.
 
 (6)) Use anti_join() in multiple times to make sure there's no missing row.
 
 (7)) Merge 3 datasets.
 
 (8)) Clean the merged dataset: Select key variables, filter out rows where result is NA.
 
 (9)) Export the final dataset.
 

Relevant code below:
```{r}
library(dplyr)
library(tidyr)
bakers = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/bakers.csv",colClasses = c("character",  # Baker Name
                                  "character",  # Series
                                  "integer",    # Baker Age
                                  "character",  # Baker Occupation
                                  "character"   # Hometown
                                  ))
bakes = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/bakes.csv", 
                  colClasses = c("character",  # Series
                                 "character",  # Episode
                                 "character",  # Baker
                                 "character",  # Signature Bake
                                 "character"   # Show Stopper
                                 ))
results = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/results.csv", skip = 1,
                    colClasses = c("character",  # Series
                                   "character",  # Episode
                                   "character",  # Baker
                                   "character",  # Technical
                                   "character"   # Result
                                   ))
viewers = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/viewers.csv", 
                    colClasses = c("character",  # Episode
                                   rep("numeric", 10)
                                   ))
bakers = janitor::clean_names(bakers)
bakes = janitor::clean_names(bakes)
results = janitor::clean_names(results)
viewers = janitor::clean_names(viewers)
colnames(results)
# Rename columns from results:
results = results |>
rename(series = x, episode = x_1, baker = x_2, technical=x_3, result = x_4)
# Create variable "baker" as the first name in Bakers dataset:
bakers = bakers |> mutate(baker = sub(" .*", "", `baker_name`)) 
# Delete "" of "Jo" in Bakes dataset:
bakes = bakes |> mutate(baker = gsub('"', '', baker))
# Check the datasets:
missing_bakers <- anti_join(bakes, bakers, by = c("series", "baker"))
print(missing_bakers) #No missing row here#
# Merge the datasets:
merged = bakes |>
left_join(bakers, by = c("baker", "series"))
merged = merged |>
left_join(results, by = c("series", "episode", "baker"))
# Clean and tidy the dataset:
final = merged |>
select(series, episode, baker_name, baker_age, hometown, baker_occupation,  signature_bake, show_stopper, result) |>
filter(!is.na(result))
# Export the final cleaned dataset
write.csv(final, "final_bakeoff_data.csv", row.names = FALSE)
```

The final dataset, "final_bakeoff_data", contains key information from the three merged tables: bakers, bakes, and results which has `r ncol(final)` variables and `r nrow(final)` observations. The baker_name, baker_age, hometown, and baker_occupation columns provide background on each contestant, while signature_bake, show_stopper, and result summarize their performance. All columns have consistent data types after processing. Rows with missing result values were filtered out, ensuring that only complete records are included. The dataset is clean and ready for analysis, with relevant details from each stage of the bake-off competition.

2. Create a table showing the star baker or winner of each episode in Seasons 5 through 10: 
```{r}
# Filter the dataset for Seasons 5 through 10
seasons_5_to_10 = final |> filter(as.numeric(series) >= 5 & as.numeric(series) <= 10)
# Extract episodes where the result is "Star Baker" or "Winner"
star_baker_winner <- seasons_5_to_10 %>%
filter(result %in% c("STAR BAKER", "WINNER")) %>%
select(series, episode, baker_name, result) %>%
arrange(series, episode)
print(star_baker_winner)
```

Comments:

In Seasons 6, 7, and 8, the winners (Nadiya Hussain, Candice Brown, and Sophie Faldo) each had multiple "Star Baker" titles, showing strong performances throughout their seasons. Their consistent wins, particularly in the later episodes, made their overall victories predictable.

The biggest surprise was in Season 5, where Richard Burr won "Star Baker" five times but lost the competition to Nancy Birtwhistle, who only had one "Star Baker" win. Despite Richard's dominance, the final outcome was unexpected.

3. Import, clean, tidy, and organize the viewership data in viewers.csv:
```{r}
library(dplyr)
library(tidyr)
viewers = read.csv("C:/Users/22865/OneDrive/Desktop/Data Science 1/HWs/HW2/gbb_datasets/viewers.csv", 
                   colClasses = c("character", rep("numeric", 10)))
viewers = janitor::clean_names(viewers)
# Pivot the data to longer format for easier analysis:
tidy_viewers = viewers |>
  pivot_longer(cols = starts_with("series"),
               names_to = "series",
               names_prefix = "series_",
               values_to = "viewership")
# Convert episode to numeric for better sorting:
tidy_viewers = tidy_viewers |> mutate(episode = as.numeric(episode))
# Organize the data by series and episode:
organized_viewers = tidy_viewers |> arrange(as.numeric(series), episode)
# Display the first 10 rows:
head(organized_viewers, 10)

# Calculate average viewership for Season 1
average_viewership_season_1 = organized_viewers |>
filter(series == "1") |>
summarise(avg_viewership = mean(viewership, na.rm = TRUE))
# Calculate average viewership for Season 5
average_viewership_season_5 = organized_viewers |>
filter(series == "5") |>
summarise(avg_viewership = mean(viewership, na.rm = TRUE))
```

By analyzing the organized viewers dataset:

The average viewership in Season 1 is `r average_viewership_season_1`, and is `r average_viewership_season_5` in Season 5.







